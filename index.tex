% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Decision Tree Challenge},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Decision Tree Challenge}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Feature Importance and Categorical Variable Encoding}
\author{}
\date{}
\begin{document}
\maketitle


\section{ðŸŒ³ Decision Tree Challenge - Feature Importance and Variable
Encoding}\label{decision-tree-challenge---feature-importance-and-variable-encoding}

\subsection{Challenge Overview}\label{challenge-overview}

\textbf{Your Mission:} Create a simple GitHub Pages site that
demonstrates how decision trees measure feature importance and analyzes
the critical differences between categorical and numerical variable
encoding. You'll answer two key discussion questions by adding narrative
to a pre-built analysis and posting those answers to your GitHub Pages
site as a rendered HTML document.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-warning-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-warning-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{âš ï¸ AI Partnership Required}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

This challenge pushes boundaries intentionally. You'll tackle problems
that normally require weeks of study, but with Cursor AI as your partner
(and your brain keeping it honest), you can accomplish more than you
thought possible.

\textbf{The new reality:} The four stages of competence are Ignorance â†’
Awareness â†’ Learning â†’ Mastery. AI lets us produce Mastery-level work
while operating primarily in the Awareness stage. I focus on awareness
training, you leverage AI for execution, and together we create outputs
that used to require years of dedicated study.

\end{tcolorbox}

\subsection{The Decision Tree Problem
ðŸŽ¯}\label{the-decision-tree-problem}

\begin{quote}
``The most important thing in communication is hearing what isn't
said.'' - Peter Drucker
\end{quote}

\textbf{The Core Problem:} Decision trees are often praised for their
interpretability and ability to handle both numerical and categorical
variables. But what happens when we encode categorical variables as
numbers? How does this affect our understanding of feature importance?

\textbf{What is Feature Importance?} In decision trees, feature
importance measures how much each variable contributes to reducing
impurity (or improving prediction accuracy) across all splits in the
tree. It's a key metric for understanding which variables matter most
for your predictions.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-important-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-important-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{ðŸŽ¯ The Key Insight: Encoding Matters for Interpretability}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\textbf{The problem:} When we encode categorical variables as numerical
values (like 1, 2, 3, 4\ldots), decision trees treat them as if they
have a meaningful numerical order. This can completely distort our
analysis.

\textbf{The Real-World Context:} In real estate, we know that
neighborhood quality, house style, and other categorical factors are
crucial for predicting home prices. But if we encode these as numbers,
we might get misleading insights about which features actually matter
most.

\textbf{The Devastating Reality:} Even sophisticated machine learning
models can give us completely wrong insights about feature importance if
we don't properly encode our variables. A categorical variable that
should be among the most important might appear irrelevant, while a
numerical variable might appear artificially important.

\end{tcolorbox}

Let's assume we want to predict house prices and understand which
features matter most. The key question is: \textbf{How does encoding
categorical variables as numbers affect our understanding of feature
importance?}

\subsection{The Ames Housing Dataset ðŸ }\label{the-ames-housing-dataset}

We are analyzing the Ames Housing dataset which contains detailed
information about residential properties sold in Ames, Iowa from 2006 to
2010. This dataset is perfect for our analysis because it contains a
categorical variable (like zip code) and numerical variables (like
square footage, year built, number of bedrooms).

\subsection{The Problem: ZipCode as Numerical vs
Categorical}\label{the-problem-zipcode-as-numerical-vs-categorical}

\textbf{Key Question:} What happens when we treat zipCode as a numerical
variable in a decision tree? How does this affect feature importance
interpretation?

\textbf{The Issue:} Zip codes (50010, 50011, 50012, 50013) are
categorical variables representing discrete geographic areas,
i.e.~neighborhoods. When treated as numerical, the tree might split on
``zipCode \textgreater{} 50012.5'' - which has no meaningful
interpretation for house prices. Zip codes are non-ordinal categorical
variables meaning they have no inherent order that aids house price
prediction (i.e.~zip code 99999 is not the priceiest zip code).

\subsection{Data Loading and Model
Building}\label{data-loading-and-model-building}

\subsubsection{Python}\label{python}

\phantomsection\label{load-and-model-python}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeRegressor, plot\_tree}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error, r2\_score}
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{\textquotesingle{}ignore\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load data}
\NormalTok{sales\_data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"salesPriceData.csv"}\NormalTok{)}

\CommentTok{\# Prepare model data (treating zipCode as numerical)}
\NormalTok{model\_vars }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}SalePrice\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}LotArea\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}YearBuilt\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}GrLivArea\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}FullBath\textquotesingle{}}\NormalTok{, }
              \StringTok{\textquotesingle{}HalfBath\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}BedroomAbvGr\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}TotRmsAbvGrd\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}GarageCars\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}zipCode\textquotesingle{}}\NormalTok{]}
\NormalTok{model\_data }\OperatorTok{=}\NormalTok{ sales\_data[model\_vars].dropna()}

\CommentTok{\# Split data}
\NormalTok{X }\OperatorTok{=}\NormalTok{ model\_data.drop(}\StringTok{\textquotesingle{}SalePrice\textquotesingle{}}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ model\_data[}\StringTok{\textquotesingle{}SalePrice\textquotesingle{}}\NormalTok{]}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Build decision tree}
\NormalTok{tree\_model }\OperatorTok{=}\NormalTok{ DecisionTreeRegressor(max\_depth}\OperatorTok{=}\DecValTok{3}\NormalTok{, }
\NormalTok{                                  min\_samples\_split}\OperatorTok{=}\DecValTok{20}\NormalTok{, }
\NormalTok{                                  min\_samples\_leaf}\OperatorTok{=}\DecValTok{10}\NormalTok{, }
\NormalTok{                                  random\_state}\OperatorTok{=}\DecValTok{123}\NormalTok{)}
\NormalTok{tree\_model.fit(X\_train, y\_train)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Model built with }\SpecialCharTok{\{}\NormalTok{tree\_model}\SpecialCharTok{.}\NormalTok{get\_n\_leaves()}\SpecialCharTok{\}}\SpecialStringTok{ terminal nodes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Model built with 8 terminal nodes
\end{verbatim}

\subsection{Tree Visualization}\label{tree-visualization}

\subsubsection{Python}\label{python-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize tree}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plot\_tree(tree\_model, }
\NormalTok{          feature\_names}\OperatorTok{=}\NormalTok{X\_train.columns,}
\NormalTok{          filled}\OperatorTok{=}\VariableTok{True}\NormalTok{, }
\NormalTok{          rounded}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          fontsize}\OperatorTok{=}\DecValTok{10}\NormalTok{,}
\NormalTok{          max\_depth}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Decision Tree (zipCode as Numerical)"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/visualize-tree-python-output-1.pdf}}

:::

\subsection{Feature Importance
Analysis}\label{feature-importance-analysis}

\subsubsection{Python}\label{python-2}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/importance-plot-python-output-1.pdf}}

\subsection{Critical Analysis: The Encoding
Problem}\label{critical-analysis-the-encoding-problem}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-warning-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-warning-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{âš ï¸ The Problem Revealed}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\textbf{What to note:} Our decision tree treated \texttt{zipCode} as a
numerical variable. This leads to zip code being unimportant. Not
surprisingly, because there is no reason to believe allowing splits like
``zipCode \textless{} 50012.5'' should be beneficial for house price
prediction. This false coding of a variable creates several problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Potentially Meaningless Splits:} A zip code of 50013 is not
  ``greater than'' 50012 in any meaningful way for house prices
\item
  \textbf{False Importance:} The algorithm assigns importance to zipCode
  based on numerical splits rather than categorical distinctions OR the
  importance of zip code is completely missed as numerical ordering has
  no inherent relationship to house prices.
\item
  \textbf{Misleading Interpretations:} We might conclude zipCode is not
  important when our intuition tells us it should be important (listen
  to your intuition).
\end{enumerate}

\textbf{The Real Issue:} Zip codes are categorical variables
representing discrete geographic areas. The numerical values have no
inherent order or magnitude relationship to house prices. These must be
modelled as categorical variables.

\end{tcolorbox}

\subsection{Proper Categorical Encoding: The
Solution}\label{proper-categorical-encoding-the-solution}

Now let's repeat the analysis with zipCode properly encoded as
categorical variables to see the difference.

\textbf{Python Approach:} One-hot encode zipCode (create dummy variables
for each zip code)

\subsubsection{Categorical Encoding
Analysis}\label{categorical-encoding-analysis}

\subsubsection{Python}\label{python-3}

\subsubsection{Tree Visualization: Categorical
zipCode}\label{tree-visualization-categorical-zipcode}

\subsubsection{Python}\label{python-4}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize tree with one{-}hot encoded zipCode}
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\NormalTok{plot\_tree(tree\_model\_cat, }
\NormalTok{          feature\_names}\OperatorTok{=}\NormalTok{X\_train\_cat.columns,}
\NormalTok{          filled}\OperatorTok{=}\VariableTok{True}\NormalTok{, }
\NormalTok{          rounded}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{          fontsize}\OperatorTok{=}\DecValTok{8}\NormalTok{,}
\NormalTok{          max\_depth}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Decision Tree (zipCode One{-}Hot Encoded)"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/visualize-tree-cat-python-output-1.pdf}}

\subsubsection{Feature Importance: Categorical
zipCode}\label{feature-importance-categorical-zipcode}

\subsubsection{Python}\label{python-5}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/importance-plot-cat-python-output-1.pdf}}

\subsection{Challenge Requirements ðŸ“‹}\label{challenge-requirements}

\subsubsection{Minimum Requirements for Any Points on
Challenge}\label{minimum-requirements-for-any-points-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Create a GitHub Pages Site:} Use the starter repository (see
  Repository Setup section below) to begin with a working template. The
  repository includes all the analysis code and visualizations above.
\item
  \textbf{Add Discussion Narrative:} Add your answers to the two
  discussion questions below in the Discussion Questions section of the
  rendered HTML.
\item
  \textbf{GitHub Repository:} Use your forked repository (from the
  starter repository) named ``decTreeChallenge'' in your GitHub account.
\item
  \textbf{GitHub Pages Setup:} The repository should be made the source
  of your github pages:

  \begin{itemize}
  \tightlist
  \item
    Go to your repository settings (click the ``Settings'' tab in your
    GitHub repository)
  \item
    Scroll down to the ``Pages'' section in the left sidebar
  \item
    Under ``Source'', select ``Deploy from a branch''
  \item
    Choose ``main'' branch and ``/ (root)'' folder
  \item
    Click ``Save''
  \item
    Your site will be available at:
    \texttt{https://{[}your-username{]}.github.io/decTreeChallenge/}
  \item
    \textbf{Note:} It may take a few minutes for the site to become
    available after enabling Pages
  \end{itemize}
\end{enumerate}

\subsection{Getting Started: Repository Setup
ðŸš€}\label{getting-started-repository-setup}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-important-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-important-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{ðŸ“ Quick Start with Starter Repository}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\textbf{Step 1:} Fork the starter repository to your github account at
\url{https://github.com/flyaflya/decTreeChallenge.git}

\textbf{Step 2:} Clone your fork locally using Cursor (or VS Code)

\textbf{Step 3:} You're ready to start! The repository includes
pre-loaded data and a working template with all the analysis above.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-tip-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-tip-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{ðŸ’¡ Why Use the Starter Repository?}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\textbf{Benefits:}

\begin{itemize}
\tightlist
\item
  \textbf{Pre-loaded data:} All required data and analysis code is
  included
\item
  \textbf{Working template:} Basic Quarto structure (\texttt{index.qmd})
  is ready
\item
  \textbf{No setup errors:} Avoid common data loading issues
\item
  \textbf{Focus on analysis:} Spend time on the discussion questions,
  not data preparation
\end{itemize}

\end{tcolorbox}

\subsubsection{Getting Started Tips}\label{getting-started-tips}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-note-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-note-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ðŸŽ¯ Navy SEALs Motto}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\begin{quote}
``Slow is Smooth and Smooth is Fast''
\end{quote}

\emph{Take your time to understand the decision tree mechanics, plan
your approach carefully, and execute with precision. Rushing through
this challenge will only lead to errors and confusion.}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-warning-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-warning-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{ðŸ’¾ Important: Save Your Work Frequently!}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\textbf{Before you start:} Make sure to commit your work often using the
Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This
prevents the AI from overwriting your progress and ensures you don't
lose your work.

\textbf{Commit after each major step:}

\begin{itemize}
\tightlist
\item
  After adding your discussion answers
\item
  After rendering to HTML
\item
  Before asking the AI for help with new code
\end{itemize}

\textbf{How to commit:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open Source Control panel (Ctrl+Shift+G)
\item
  Stage your changes (+ button)
\item
  Write a descriptive commit message
\item
  Click the checkmark to commit
\end{enumerate}

\emph{Remember: Frequent commits are your safety net!}

\end{tcolorbox}

\subsection{Discussion Questions for
Challenge}\label{discussion-questions-for-challenge}

\textbf{Your Task:} Add thoughtful narrative answers to these two
questions in the Discussion Questions section of your rendered HTML
site.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Numerical vs Categorical Encoding:} There are two modelsin
  Python written above. For each language, the models differ by how zip
  code is modelled, either as a numerical variable or as a categorical
  variable. Given what you know about zip codes and real estate prices,
  how should zip code be modelled, numerically or categorically? Is
  zipcode and ordinal or non-ordinal variable?
\item
  \textbf{R vs Python Implementation Differences:} When modelling zip
  code as a categorical variable, the output tree and feature importance
  would differ quite significantly had you used R as opposed to Python.
  Investigate why this is the case. What does R offer that Python does
  not? Which language would you say does a better job of modelling zip
  code as a categorical variable? Can you quote the documentation at
  \url{https://scikit-learn.org/stable/modules/tree.html} suggesting a
  weakness in the Python implementation? If so, please provide a quote
  from the documentation.
\item
  \textbf{Are There Any Suggestions for Implementing Decision Trees in
  Python With Prioper Categorical Handling?} Please poke around the
  Internet (AI is not as helpful with new libraries) for suggestions on
  how to implement decision trees in Python with better (i.e.~not
  one-hot encoding) categorical handling. Please provide a link to the
  source and a quote from the source. There is not right answer here,
  but please provide a thoughtful answer, I am curious to see what you
  find.
\end{enumerate}

\subsection{Grading Rubric ðŸŽ“}\label{grading-rubric}

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, coltitle=black, colframe=quarto-callout-important-color-frame, opacitybacktitle=0.6, colbacktitle=quarto-callout-important-color!10!white, leftrule=.75mm, left=2mm, breakable, bottomtitle=1mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{ðŸ“Š What You're Really Being Graded On}, titlerule=0mm, colback=white, toptitle=1mm, arc=.35mm, bottomrule=.15mm, toprule=.15mm, opacityback=0]

\textbf{This is an investigative report, not a coding exercise.} You're
analyzing decision tree models and reporting your findings like a
professional analyst would. Think of this as a brief you'd write for a
client or manager about why proper variable encoding matters in machine
learning.

\textbf{What makes a great report:}

\begin{itemize}
\tightlist
\item
  \textbf{Clear narrative:} Tell the story of what you discovered about
  decision tree feature importance
\item
  \textbf{Insightful analysis:} Focus on the most interesting
  differences between numerical and categorical encoding
\item
  \textbf{Professional presentation:} Clean, readable, and engaging
\item
  \textbf{Concise conclusions:} No AI babble or unnecessary technical
  jargon
\item
  \textbf{Human insights:} Your interpretation of what the feature
  importance rankings actually mean (or don't mean)
\item
  \textbf{Documentation-based analysis:} For question 2, ground your
  analysis in actual library documentation
\end{itemize}

\textbf{What we're looking for:} A compelling 1-2 minute read that
demonstrates both the power of decision trees for interpretability and
the critical importance of proper variable encoding. And a note on the
current state of the art in decision tree implementation for categorical
variables in Python.

\end{tcolorbox}

\subsubsection{Questions to Answer for 75\% Grade on
Challenge}\label{questions-to-answer-for-75-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Numerical vs Categorical Analysis:} Provide a clear,
  well-reasoned answer to question 1 about how zip codes should be
  modelled. Your answer should demonstrate understanding of why
  categorical variables need special treatment in decision trees.
\end{enumerate}

\subsubsection{Questions to Answer for 85\% Grade on
Challenge}\label{questions-to-answer-for-85-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{R vs Python Implementation Analysis:} Provide a thorough
  analysis of question 2, including investigation of the official
  documentation for both \texttt{rpart} (R) and
  \texttt{sklearn.tree.DecisionTreeRegressor} (Python). Your analysis
  should explain the technical differences and provide a reasoned
  opinion about which implementation handles categorical variables
  better. You do NOT have to run R-code.
\end{enumerate}

\subsubsection{Questions to Answer for 95\% - 100\% Grade on
Challenge}\label{questions-to-answer-for-95---100-grade-on-challenge}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Professional Presentation:} Your discussion answers should be
  written in a professional, engaging style that would be appropriate
  for a business audience learning about one-hot encoding and decision
  trees. Avoid technical jargon and focus on practical implications.
  Include a specific quote from the official documentation of
  \texttt{sklearn.tree.DecisionTreeRegressor} that supports your
  analysis.
\end{enumerate}

\subsection{Submission Checklist âœ…}\label{submission-checklist}

\textbf{Minimum Requirements (Required for Any Points):}

\begin{itemize}
\tightlist
\item[$\square$]
  Forked starter repository from
  \url{https://github.com/flyaflya/decTreeChallenge.git}
\item[$\square$]
  Cloned repository locally using Cursor (or VS Code)
\item[$\square$]
  Added thoughtful narrative answers to both discussion questions
\item[$\square$]
  Document rendered to HTML successfully
\item[$\square$]
  HTML files uploaded to your forked repository
\item[$\square$]
  GitHub Pages enabled and working
\item[$\square$]
  Site accessible at
  \texttt{https://{[}your-username{]}.github.io/decTreeChallenge/}
\end{itemize}

\textbf{75\% Grade Requirements:}

\begin{itemize}
\tightlist
\item[$\square$]
  Clear, well-reasoned answer to question 1 about numerical vs
  categorical encoding
\end{itemize}

\textbf{85\% Grade Requirements:}

\begin{itemize}
\tightlist
\item[$\square$]
  Thorough analysis of question 2 with investigation of official
  documentation
\end{itemize}

\textbf{95\% Grade Requirements:}

\begin{itemize}
\tightlist
\item[$\square$]
  Professional presentation style appropriate for business audience.
\item[$\square$]
  Specific quote from official documentation of
  \texttt{sklearn.tree.DecisionTreeRegressor} supporting your analysis
\end{itemize}

\textbf{100\% Grade Requirements:}

\begin{itemize}
\tightlist
\item[$\square$]
  Note on the current state of the art in decision tree implementation
  for categorical variables in Python.
\end{itemize}

\textbf{Report Quality (Critical for Higher Grades):}

\begin{itemize}
\tightlist
\item[$\square$]
  Clear, engaging narrative that tells a story
\item[$\square$]
  Focus on the most interesting findings about decision tree feature
  importance
\item[$\square$]
  Professional writing style (no AI-generated fluff)
\item[$\square$]
  Concise analysis that gets to the point
\item[$\square$]
  Practical insights that would help a real data scientist
\item[$\square$]
  Documentation-based analysis for technical questions
\end{itemize}




\end{document}
